{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1a6b73-bc2f-477e-9dac-4cda7bb88143",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "#### Math 3480 - Machine Learning - Dr. Michael E. Olson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b337c",
   "metadata": {},
   "source": [
    "## Reading\n",
    "* Geron, Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739e7aa",
   "metadata": {},
   "source": [
    "Decision Trees are efficient, but often have poor results. To improve the results, we aggregate the information, or look at groups of predictors. This group of predictors is called an __ensemble__.\n",
    "\n",
    "Applying this to Decision Trees,\n",
    "* Take a random subset of the data (Bagging)\n",
    "* Run it through a Decision Tree\n",
    "* Run it through a different Decision Tree\n",
    "* Continue until you have run the data subset through all the trees\n",
    "* Use this aggregated information to predict the class that gets the most votes\n",
    "\n",
    "This process, since we are looking through multiple trees, is called a __Random Forest__, and is one of the most powerful Machine Learning algorithms available today.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790f904",
   "metadata": {},
   "source": [
    "### Voting\n",
    "One way we can get the best results is to run the data through the different methods (Logistic Regression, SVM, Random Forests, KNN,...) and take the model that performs the best\n",
    "* Run the data through each ML algorithm\n",
    "* Aggregate (or collect) the data together\n",
    "* Take a majority vote on which method has the most correct classifications, and aggregate the results\n",
    "\n",
    "The majority-vote classifier is called *hard voting*.\n",
    "\n",
    "*Soft voting* would be predicting the class with the highest class probability averaged over all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818048f0",
   "metadata": {},
   "source": [
    "### Bagging (Bootstrap AGGregatING)\n",
    "To bootstrap (or create a subset of your dataset):\n",
    "* Take a random element from your data, and include it in your dataset\n",
    "  * with replacement (it's not removed from the original dataset, so it could be selected again later)\n",
    "* Continue to take a determined number of elements from your dataset, with replacement\n",
    "  * If you repeat some elements, that is just fine - it is useful in the algorithm\n",
    "  * How many elements in each bag?\n",
    "    * If $n$ is the number of elements in the original dataset, and $n'$ is the number in the bag, we want $n'<n$\n",
    "    * A good round figure would be $n' \\approx 60\\% \\cdot n$. Not a hard number - can vary based on need\n",
    "\n",
    "We will then use each bag to be trained into models. We then apply our data to each model and take the average for our result. This is the __ensemble__ process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e4ef8",
   "metadata": {},
   "source": [
    "#### How to do Bagging in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 6)\n",
    "from sklearn import datasets\n",
    "\n",
    "iris_data = datasets.load_iris()\n",
    "iris = pd.DataFrame(iris_data['data'], columns=iris_data['feature_names'])\n",
    "species = pd.DataFrame(iris_data['target'], columns=['species_num'])\n",
    "\n",
    "def test_species(x):\n",
    "    if x==0: return \"setosa\"\n",
    "    if x==1: return \"versicolor\"\n",
    "    if x==2: return \"virginica\"\n",
    "\n",
    "iris['species'] = species['species_num'].apply(lambda x: test_species(x))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.drop(['species'],axis=1), iris['species'], test_size=0.3, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "    \n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_predict = bag_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_train,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16d163",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "* Bootstrap your data\n",
    "* Using the bootstrapped data, select only a few variables to test\n",
    "* Determine which variable creates the best split in the data - this is your first node\n",
    "* Select only a few of the remaining variables to test\n",
    "* The variable that creates the best split becomes the second node\n",
    "* ...etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa8c9f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
