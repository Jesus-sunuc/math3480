{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835ef8c9",
   "metadata": {},
   "source": [
    "# Entropy and Information Gain\n",
    "#### Math 3480 - Machine Learning\n",
    "#### Dr. Michael E. Olson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6433e1",
   "metadata": {},
   "source": [
    "# Reading\n",
    "* Geron, Chapter 6\n",
    "\n",
    "## Additional Resources\n",
    "* [YouTube: Shannon Entropy and Information Gain](https://www.youtube.com/watch?v=9r7FIXEAGvs) by Serrano Academy\n",
    "* [YouTube: Entropy (for data science) Clearly Explained!!!](https://www.youtube.com/watch?v=7VeUPuFGJHk&t=143s) by StatQuest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24b603",
   "metadata": {},
   "source": [
    "# Recall\n",
    "Expected values are calculated as\n",
    "$$E(x) = \\sum\\left(x\\cdot P(x)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02e6ef",
   "metadata": {},
   "source": [
    "# Introduction Scenario\n",
    "* Choose 4 balls out of a bucket of red balls\n",
    "  * How surprised are you if you get RRRR? Not\n",
    "* Choose 4 balls out of a bucket of 75% red 25% blue balls\n",
    "  * How surprised are you if you get RRRR? Moderately\n",
    "* Choose 4 balls out of a bucket of 50% red 50% blue balls\n",
    "  * How surprised are you if you get RRBB? High"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b22cb3",
   "metadata": {},
   "source": [
    "# Surprise\n",
    "Surprise and likelihood (probability) are inversely proportional. So, it's tempting to say, $Surprise = \\frac{1}{P(x)}$. But it's deceptive, because following this, $P(x)=1$ happens when you have the most surprise.\n",
    "\n",
    "Try a logarithm. Then $P(x)=0$ will give the most surprise\n",
    "$$\\log_2\\left(\\frac{1}{P(x)}\\right) = -\\log_2P(x)$$\n",
    "* Why $\\log_2$? When we have 2 possibile outcomes, we have a binary situation\n",
    "* The number of possible outcomes for drawing 4 balls is $2*2*2*2=2^4$\n",
    "* The number of possible outcomes for drawing $k$ balls is $2*2*2*2=2^k$\n",
    "\n",
    "Because of this, we are going to use $\\log_2$. The base may change if there are a different number of outcomes, but in a decision tree, we only have two outcomes.\n",
    "\n",
    "__What is the surprise when drawing 4 balls when the bin is 75% red and 25% blue?__ The probability of this happening is,\n",
    "$$P(x) = 0.75*0.75*0.75*0.25 = 0.1055$$\n",
    "\n",
    "Fairly low probability. The surprise is then,\n",
    "$$Surprise = -\\log_2 (0.75*0.75*0.75*0.25) = -\\log_2 0.75 + -\\log_2 0.75 + -\\log_2 0.75 + -\\log_2 0.25$$\n",
    "This is just the sum of the surprise of each event!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226fccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2451124978365313"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "-np.log2(0.75) - np.log2(0.75) - np.log2(0.75) - np.log2(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddecda1",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "__Entropy__ is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty\n",
    "* Entropy is the Expected Value of the Surprise\n",
    "\n",
    "To find the entropy, compare surprise to probability\n",
    "* Find the product for each outcome\n",
    "$$-P(x)\\log_2 P(x)$$\n",
    "* Add them together, and we get entropy!\n",
    "$$E(surprise) = Entropy = \\sum -P(x)\\log_2 P(x)$$\n",
    "\n",
    "Note that this is just the equation for the expected value: $E(x)=\\sum x\\cdot P(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76393fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = -0.00\n"
     ]
    }
   ],
   "source": [
    "# 4 balls from a bucket of red balls\n",
    "# P(R) = 1     surprise(R) = log2(1) = 0\n",
    "# P(B) = 0     surprise(B) = log2(0) = infinite\n",
    "\n",
    "# - [ P(R) * log2 P(R) ] - [ P(B) * log2 P(B) ]\n",
    "E = -1 * np.log2(1)\n",
    "print(\"Entropy = {0:.2f}\".format(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64550f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 0.81\n"
     ]
    }
   ],
   "source": [
    "# 4 balls from a bucket of 75% red balls and 25% blue balls\n",
    "# P(R) = 0.75     surprise(R) = log2(0.75)\n",
    "# P(B) = 0.25     surprise(B) = log2(0.25)\n",
    "\n",
    "# - [ P(R) * log2 P(R) ] - [ P(B) * log2 P(B) ]\n",
    "E = -0.75 * np.log2(0.75) - 0.25 * np.log2(0.25)\n",
    "print(\"Entropy = {0:.2f}\".format(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86616fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 1.00\n"
     ]
    }
   ],
   "source": [
    "# 4 balls from a bucket of 50% red balls and 50% blue balls\n",
    "# P(R) = 0.50     surprise(R) = log2(0.50)\n",
    "# P(B) = 0.50     surprise(B) = log2(0.50)\n",
    "\n",
    "# - [ P(R) * log2 P(R) ] - [ P(B) * log2 P(B) ]\n",
    "E = -0.5 * np.log2(0.5) - 0.5 * np.log2(0.5)\n",
    "print(\"Entropy = {0:.2f}\".format(E))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafcc72d",
   "metadata": {},
   "source": [
    "# Information Gain\n",
    "When we leave things to probability, then our knowledge is lower and entropy is higher. When the knowledge is higher, then entropy is lower.\n",
    "* Information gain is the reduction in entropy or surprise by transforming a dataset\n",
    "\n",
    "To calculate the Information gain,\n",
    "$$IG(x,A)=H(x)-\\sum_{v\\in Values(A)} \\frac{|x_v|}{x}H(x_v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c18f9",
   "metadata": {},
   "source": [
    "# Another example\n",
    "Now, 8 letters, A-D. Find the entropy of each:\n",
    "1. AAAAAAAA\n",
    "2. AAAABBCD\n",
    "3. AABBCCDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17bba9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = -0.00. Since we know that each character has to be an A, our knowledge is high.\n"
     ]
    }
   ],
   "source": [
    "# AAAAAAAA\n",
    "# P(A) = 1\n",
    "# P(B) = 0\n",
    "# P(C) = 0\n",
    "# P(D) = 0\n",
    "\n",
    "E = -1 * np.log2(1)\n",
    "print(\"Entropy = {0:.2f}. Since we know that each character has to be an A, our knowledge is high.\".format(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd78b9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 1.75. Since we only know that half the characters are an A, our knowledge is moderate.\n"
     ]
    }
   ],
   "source": [
    "# AAAABBCD\n",
    "# P(A) = 0.50\n",
    "# P(B) = 0.25\n",
    "# P(C) = 0.125\n",
    "# P(D) = 0.125\n",
    "\n",
    "E = -0.50 * np.log2(0.50) - 0.25 * np.log2(0.25) - 0.125 * np.log2(0.125) - 0.125 * np.log2(0.125)\n",
    "print(\"Entropy = {0:.2f}. Since we only know that half the characters are an A, our knowledge is moderate.\".format(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90404e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 2.00. Since we know that each character has to be an A, our knowledge is high.\n"
     ]
    }
   ],
   "source": [
    "# AABBCCDD\n",
    "# P(A) = 0.25\n",
    "# P(B) = 0.25\n",
    "# P(C) = 0.25\n",
    "# P(D) = 0.25\n",
    "\n",
    "E = -0.25 * np.log2(0.25) - 0.25 * np.log2(0.25) - 0.25 * np.log2(0.25) - 0.25 * np.log2(0.25)\n",
    "print(\"Entropy = {0:.2f}. Since we know that each character has to be an A, our knowledge is high.\".format(E))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9651b68",
   "metadata": {},
   "source": [
    "# Another look at Entropy\n",
    "Another way to define Entropy:\n",
    "* Entropy is the average number of questions we need to ask to get a certain result\n",
    "\n",
    "__Take the first example: $AAAAAAAA$__\n",
    "* How many questions do you need to ask to know which letter you drew?\n",
    "* Since $P(A) = 1$, we don't have to ask any\n",
    "  * Questions asked = 0 = Entropy\n",
    "\n",
    "__Take the third example: $AABBCCDD$__\n",
    "* How many questions do you need to ask to know which letter you drew?\n",
    "  * We could ask \n",
    "    * Is it A?\n",
    "    * Is it B?\n",
    "    * Is it C?\n",
    "    * At this point, we know it's not A, B, or C, so we don't have to ask D. So, we asked 3 questions.\n",
    "* We can look at this a different way. Ask instead,\n",
    "  * Is it A or B?\n",
    "  * Consider the diagram:\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNpdkMEKgzAMhl8l5Kwv4GFDqxuDscN2GtZDsXEW1EptD0N991XmQJdT-L8vkGTEUkvCCF9G9DVc77wDX_F4GUBZIGVrMhCDNpAc5xVCGB5getIwQbKK8R-86QnYytiPJZvBLI8L2MXLSJonxTdkG_eUs326qOc8LTDAlkwrlPQXjIvC0S_cEsfIt5Iq4RrLkXezV10vhaVMKqsNRpVoBgpQOKsf767EyBpHPylVwj-kXa35AzbKWTA)](https://mermaid-js.github.io/mermaid-live-editor/edit/#pako:eNpdkMEKgzAMhl8l5Kwv4GFDqxuDscN2GtZDsXEW1EptD0N991XmQJdT-L8vkGTEUkvCCF9G9DVc77wDX_F4GUBZIGVrMhCDNpAc5xVCGB5getIwQbKK8R-86QnYytiPJZvBLI8L2MXLSJonxTdkG_eUs326qOc8LTDAlkwrlPQXjIvC0S_cEsfIt5Iq4RrLkXezV10vhaVMKqsNRpVoBgpQOKsf767EyBpHPylVwj-kXa35AzbKWTA)\n",
    "\n",
    "In each case, it took 2 questions. Questions asked = $\\sum n\\cdot P(x) = n_AP(A) + n_BP(B) + n_CP(C) + n_DP(D) = 2*\\frac{1}{4} + 2*\\frac{1}{4} + 2*\\frac{1}{4} + 2*\\frac{1}{4} = 2 = Entropy$\n",
    "\n",
    "__Take the second example: $AAAABBCD$__\n",
    "* How many questions do you need to ask to know which letter you drew?\n",
    "  * We could follow the same process, but we'd get the same result (2 questions)\n",
    "  * Instead, notice that half of them are an A\n",
    "    * So, ask if it's an A\n",
    "    * If it's not in that half, then it's B, C, or D. Half of them are B\n",
    "    * Ask if it's a B\n",
    "  * Consider the diagram:\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNpVkEsKgzAQQK8yzLpewEWLv5ZC6aJdlSSLYMYaMEZiXBT17o2goLMaHu_BMCOWVhHG-HWyq-Hx4i2EScZ7D9pDcplXAFF0hulD_QQpS8SBPu0E2VqkW5Htipyl4kCXoliLbCuKXXFlmTjQpbixXOAJDTkjtQpHj4vC0ddkiGMcVkWVHBrPkbdzUIdOSU-F0t46jCvZ9HRCOXj7_rUlxt4NtEm5luEHZrXmP2eBVRM)](https://mermaid-js.github.io/mermaid-live-editor/edit/#pako:eNpVkEsKgzAQQK8yzLpewEWLv5ZC6aJdlSSLYMYaMEZiXBT17o2goLMaHu_BMCOWVhHG-HWyq-Hx4i2EScZ7D9pDcplXAFF0hulD_QQpS8SBPu0E2VqkW5Htipyl4kCXoliLbCuKXXFlmTjQpbixXOAJDTkjtQpHj4vC0ddkiGMcVkWVHBrPkbdzUIdOSU-F0t46jCvZ9HRCOXj7_rUlxt4NtEm5luEHZrXmP2eBVRM)\n",
    "\n",
    "This time, the questions asked = $\\sum n\\cdot P(x) = n_AP(A) + n_BP(B) + n_CP(C) + n_DP(D) = 1*\\frac{1}{2} + 2*\\frac{1}{4} + 3*\\frac{1}{8} + 3*\\frac{1}{8} = 1.75 = Entropy$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d19bfaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*0.5 + 2*0.25 + 3*0.125 + 3*0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b548e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8a940844f7f8998522eedb112310c0958420b6c917d0853ef275f15829120cc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
