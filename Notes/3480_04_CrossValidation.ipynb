{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1ed875-70a5-4e5c-9415-6b3d796971b5",
   "metadata": {},
   "source": [
    "# Lecture 4 Cross Validation\n",
    "__MATH 3480__ - Dr. Michael Olson\n",
    "\n",
    "Topics:\n",
    "* Cross Validation\n",
    "    * Leave-one-out cross validation\n",
    "    * k-fold cross validation\n",
    "* Performance Measures\n",
    "\n",
    "Reading:\n",
    "* Geron, pp. 31, 73-80, 88-97"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec914b-b9e2-4121-bbeb-ea185071ac62",
   "metadata": {},
   "source": [
    "In Exploratory Data Analysis, we need to follow these steps:\n",
    "1. Obtain and Clean the Data\n",
    "2. Wrangle the Data\n",
    "3. Look at statistical calculations\n",
    "4. Graph the data \n",
    "5. Draw conclusions and make hypotheses from (3) and (4), looking for relationships that we might use\n",
    "\n",
    "|              | Quantitative Data | Categorical Data |\n",
    "| :----------- | :---------------- | :--------------- |\n",
    "| Calculations | Mean, Mode<br>5-summary Statistics<br>Distributions (count, standard deviation/variance) | Probabilities<br>Expected Values<br>Probability/Binomial/etc. Distributions |\n",
    "| Graphs       | Histogram/KDE (kernel density estimator)<br>Boxplot/Violinplot<br>Scatterplot<br>Timeseries<br>Heatmap | Barplot<br>Pie Chart<br>Venn Diagram<br>Tree Diagram |\n",
    "\n",
    "\n",
    "In order to have data ready for modeling, we have to pre-process the data. For the pre-processing, we have a few steps, some of which we have seen:\n",
    "\n",
    "1. Take care of missing data\n",
    "2. Encoding categorical data\n",
    "3. Feature Scaling\n",
    "4. Splitting the Data (Cross Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f75570-8436-4343-a808-86e85c8a310c",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9974cf3-2446-4c9b-afd8-58172fff0c44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde7cf3-2086-4324-a259-0ed0ed8543be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data according to last lecture\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "exercise = pd.read_csv('Data/exercise.csv')\n",
    "X = exercise.drop(['Date','Weight Lost'], axis=1).values\n",
    "y = np.array(exercise['Weight Lost'])\n",
    "\n",
    "# Ordinal Encoder won't like nan values. Change to 'None'\n",
    "# This fits with data since there was 0 activity for that day\n",
    "X[:,3] = ['None' if x is np.nan else x for x in X[:,3]]\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# When putting in the columns in each imputer/encoder, indicate the column\n",
    "# of the original matrix\n",
    "  # [0]: Calories - fill missing values\n",
    "  # [1]: Exercise Type - One-hot encoding\n",
    "  # [3]: Quality of Exercise - Ordinal encoding\n",
    "\n",
    "ct = ColumnTransformer(transformers=[\n",
    "      ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean'), [0]),  # This is placed first in X\n",
    "      ('onehot', OneHotEncoder(), [1]),                                         # This is placed second in X\n",
    "      ('oe', OrdinalEncoder(categories=[['None','Low','Medium','High']]), [3])  # This is placed third in X\n",
    "    ], remainder='passthrough')                     # Remaining columns placed in order after the last encoder\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec0721-a421-4005-84ab-77ee8dd50ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e528cab-2958-45b4-86d3-22275c13861d",
   "metadata": {},
   "source": [
    "## k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edc2f9-97b3-4a2c-89e8-584b2cecea2c",
   "metadata": {},
   "source": [
    "We can improve cross validation by using __k-fold cross validation__. We do this by,\n",
    "1. Dividing data into $n$ groups (called *folds*)\n",
    "2. Train and run the model $n$ times\n",
    "    * Each run sets aside one fold for validation and uses the other $n-1$ folds for training\n",
    "    * Since it is run $n$ times, each fold with be set aside one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec96a1-0db6-4f8e-aaff-e50ad6a641a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, X, y, scoring=\"neg_mean_square_error\", cv=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe99cf-c58f-4335-abc5-ae4e63573f05",
   "metadata": {},
   "source": [
    "## Leave-one-out cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334165b-57f2-48d6-b5e1-ac544a9f890b",
   "metadata": {},
   "source": [
    "Leave-one-out cross validation (LOOCV) is an extreme version of k-fold cross validation. Instead of dividing the data in $n$ groups, we set aside one datapoint at a time. That means the model is trained on all the data in $X$ except one datapoint, then evaluated on that one datapoint. This is repeated for the next, and the next,... $len(X)$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6465543-60fb-4f07-a70d-d8954992905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, X, y, scoring=\"neg_mean_square_error\", cv=len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e80bd-6eec-4dee-b201-0e70bfc100b8",
   "metadata": {},
   "source": [
    "# Performance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae26cb5-503f-4afa-a3ad-ca033446694e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Categorical measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc9e81-8b65-46bc-ba60-2b9581ba504b",
   "metadata": {},
   "source": [
    "Ultimately, either your model worked or it didn't. We can break this evaluation into multiple key metrics:\n",
    "* Accuracy - Fraction of correct predictions\n",
    "  $$ accuracy = \\frac{\\#~correct}{\\#~of~predictions}$$\n",
    "  * Good choice for balanced classes (equal numbers of different options)\n",
    "  * Not a good choice for unbalanced classes\n",
    "    * Predict 100 pictures to be dogs - If 99 are dogs, that's 99% accuracy!\n",
    "* Recall\n",
    "  * Ability of a model to find all relevant cases\n",
    "$$recall = \\frac{\\#~true~positives}{\\#~true~positives + \\#~false~negatives}$$\n",
    "* Precision\n",
    "  * Ability to identify only the relevant cases\n",
    "$$precision = \\frac{\\#~true~positives}{\\#~true~positives + \\#~false~positives}$$\n",
    "* F1-Score\n",
    "  * Harmonic mean of precision and recall\n",
    "$$F_1 = 2*\\frac{precision\\cdot recall}{precision + recall}$$\n",
    "\n",
    "__Confusion Matrix__\n",
    "\n",
    "| Total Population | Prediction Positive | Prediction Negative |\n",
    "| ---: | :---: | :---: |\n",
    "| Condition Positive | True Positive (TP) | False Negative (FN)<br>(Type II error) |\n",
    "| Condition Negative | False Positive (FP)<br>(Type I error) | True Negative (TN) |\n",
    "\n",
    "Prevalence, True Positive Rate (TPR), False Positive Rate (FPR), etc.\n",
    "* https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n",
    "The method used depends on the situation.\n",
    "* Situation determines if we fix the false positives or the false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0efd074-0ab8-4d31-b37c-a75b2c68db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true, predicted):\n",
    "    total_correct = (true == predicted).sum()\n",
    "    return total_correct/len(true)\n",
    "\n",
    "def precision(true, predicted, category):\n",
    "    total_correct = ((true == predicted) & (predicted == category)).sum()\n",
    "    total_predicted = (predicted == category).sum()\n",
    "    return total_correct/total_predicted\n",
    "\n",
    "def recall(true, predicted, category):\n",
    "    total_correct = ((true == predicted) & (true == category)).sum()\n",
    "    total_predicted = (true == category).sum()\n",
    "    return total_correct/total_predicted\n",
    "\n",
    "def f1score(true, predicted, category):\n",
    "    p = precision(true, predicted, category)\n",
    "    r = recall(true, predicted, category)\n",
    "    return 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194ca9c-b593-45e0-95b1-d42c52926db5",
   "metadata": {},
   "source": [
    "*sci-kit learn* has this built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747fa93e-c4da-4383-9162-36faae6d22e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 0]\n",
      " [0 3 0]\n",
      " [0 0 3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.67      0.80         3\n",
      "           B       0.75      1.00      0.86         3\n",
      "           C       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.89         9\n",
      "   macro avg       0.92      0.89      0.89         9\n",
      "weighted avg       0.92      0.89      0.89         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = ['A','A','A','B','B','B','C','C','C']\n",
    "y_predicted = ['A','A','B','B','B','B','C','C','C']\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_predicted))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729a7e1-81e1-478c-987b-d8c93c20adde",
   "metadata": {},
   "source": [
    "## Quantitative Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2205e7-2494-4e15-a44c-ef64d3ae1f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
