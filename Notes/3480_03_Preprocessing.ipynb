{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03180db8",
   "metadata": {},
   "source": [
    "# Lecture 3 Preprocessing\n",
    "__MATH 3480__ - Dr. Michael Olson\n",
    "\n",
    "Reading:\n",
    "* Geron, Chapter 2, pp. 62-75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb868b",
   "metadata": {},
   "source": [
    "In Exploratory Data Analysis, we need to follow these steps:\n",
    "1. Obtain and Clean the Data\n",
    "2. Wrangle the Data\n",
    "3. Look at statistical calculations\n",
    "4. Graph the data \n",
    "5. Draw conclusions and make hypotheses from (3) and (4), looking for relationships that we might use\n",
    "\n",
    "|              | Quantitative Data | Categorical Data |\n",
    "| :----------- | :---------------- | :--------------- |\n",
    "| Calculations | Mean, Mode<br>5-summary Statistics<br>Distributions (count, standard deviation/variance) | Probabilities<br>Expected Values<br>Probability/Binomial/etc. Distributions |\n",
    "| Graphs       | Histogram/KDE (kernel density estimator)<br>Boxplot/Violinplot<br>Scatterplot<br>Timeseries<br>Heatmap | Barplot<br>Pie Chart<br>Venn Diagram<br>Tree Diagram |\n",
    "\n",
    "The goal of EDA:\n",
    "* Derive Insights\n",
    "* Generate Hypotheses\n",
    "\n",
    "In order to have data ready for modeling, we have to pre-process the data. For the pre-processing, we have a few steps, some of which we have seen:\n",
    "\n",
    "1. Take care of missing data\n",
    "2. Encoding categorical data\n",
    "3. Splitting the Data (Cross Validation)\n",
    "4. Feature Scaling\n",
    "\n",
    "We're going to look at this three ways\n",
    "\n",
    "1. Using functions as we have seen in our courses so far\n",
    "   * Additionally, how to execute these these in one command (piping)\n",
    "2. Using classes and objects (Still building this part of the lecture)\n",
    "3. Using pre-built classes in *sci-kit learn*\n",
    "   * Additionally, how to execute these these in one command (piping)\n",
    "\n",
    "-----\n",
    "\n",
    "To add to the lecture:\n",
    "* leave-one-out Cross Validation\n",
    "* k-fold Cross Validation\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb267e1",
   "metadata": {},
   "source": [
    "We will use the following dataset on weight loss in each case.\n",
    "> For a reminder on Obtaining and Loading data, look at the [MATH 3080 Notes](https://github.com/drolsonmi/math3080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "exercise = pd.read_csv('Data/exercise.csv')\n",
    "display(exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16e918",
   "metadata": {},
   "source": [
    "Looking at the data here, note that this is what we will need to do in order to use this data in a model.\n",
    "* Drop the *Date* column\n",
    "* Missing values in the *Calories* category\n",
    "    * Let's replace with a mean value\n",
    "* *Exercise Type* is a nominal variable and needs to become numerical\n",
    "    * Being a nominal variable, we don't want to just turn the categories into numbers as we don't want to unintentionally indicate an order\n",
    "    * Let's use One-hot encoding (also known as dummy variables)\n",
    "* *Quality of Exercise* is an ordinal variable and needs to become numerical\n",
    "    * Since there is an order to the categories, we can merely replace each category with a numerical value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7b660",
   "metadata": {},
   "source": [
    "## Using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6abc7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the date column\n",
    "def drop_col(x,col):\n",
    "    x.drop(col, axis=1, inplace=True)\n",
    "    return x\n",
    "\n",
    "# Function to fill in missing values\n",
    "def fill_avg(x,col):\n",
    "    x[col].replace(np.nan, x[col].mean(), inplace=True)\n",
    "    return x\n",
    "\n",
    "# One-hot encode\n",
    "def one_hot(x,col):\n",
    "    x = x.join(pd.get_dummies(x[col]).astype(int)).drop(col, axis=1)\n",
    "    return x\n",
    "\n",
    "# Ordinal Encode\n",
    "def ordinal_encode(x,col):\n",
    "    order = {\n",
    "        'None':0,\n",
    "        np.nan:0,\n",
    "        'Low':1,\n",
    "        'Medium':2,\n",
    "        'High':3\n",
    "    }\n",
    "    x[col] = x[col].map(order)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col(exercise,'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_avg(exercise,'Calories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise = one_hot(exercise,'Exercise Type')\n",
    "exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03989944",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise = ordinal_encode(exercise,'Quality of Exercise')\n",
    "exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeb1312",
   "metadata": {},
   "source": [
    "Now, our data is 100% numerical, and ready to be put into a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd3044",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Even though the data is all numerical and ready for the model, it could still potentially cause problems. For example, let's say we are looking at the housing market and want to compare the price of the house and the number of bedrooms to the square footage. The scale of the house price (around $500,000) is very different from the scale of the number of bedrooms (2-7). Since the scale for the house price is so much larger, the variation of prices is larger, and this may weigh more heavily in a model than the number of bedrooms, when the number of bedrooms may be a better indicator.\n",
    "\n",
    "To put the variables on the same scale, we apply __feature scaling__, where we scale all features so that they are all on similar scales. There are two ways to scale everything:\n",
    "1. __Standardization__ (aka, Min-Max scaling)\n",
    "* Scales all values to a range of [0,1], 0 representing the minimum, 1 the maximum\n",
    "$$x_{scaled} = \\frac{x-min}{max-min}$$\n",
    "\n",
    "2. __Normalization__\n",
    "* Scales all values based on the mean and standard deviation\n",
    "$$x_{scaled} = \\frac{x-\\bar{x}}{s}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5c5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale(x, col):\n",
    "    return (x[col] - x[col].min()) / (x[col].max() - x[col].min())\n",
    "\n",
    "def normal_scale(x, col):\n",
    "    return (x[col] - x[col].mean()) / x[col].std(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e58f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in exercise.columns:\n",
    "    exercise[c] = standard_scale(exercise, c)\n",
    "\n",
    "display(exercise.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef927b6",
   "metadata": {},
   "source": [
    "Note that any test data and any new data sent to the model has to go through the same preprocessing (one-hot encoding, ordinal encoding, feature scaling) that the training data did. For this reason, it is nice to simplify and automate the process, which is what we will discuss next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b7372",
   "metadata": {},
   "source": [
    "### Piping functions into one command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a507b0",
   "metadata": {},
   "source": [
    "We can also do all of these functions in one command. We do this by taking the output of one function and using it as the input for another function. In a very messy way, we can do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise = pd.read_csv('Data/exercise.csv')\n",
    "exercise = ordinal_encode(one_hot(fill_avg(drop_col(exercise,'Date'),'Calories'),'Exercise Type'),'Quality of Exercise')\n",
    "exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3b16d",
   "metadata": {},
   "source": [
    "However, this code is very difficult to read. So, we use __piping__ instead, which sends a dataset into a function, whose result is sent to another function, whose result is sent to another function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b64927",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise = pd.read_csv('Data/exercise.csv')\n",
    "exercise = (exercise.pipe(drop_col,'Date')\n",
    "                    .pipe(fill_avg,'Calories')\n",
    "                    .pipe(one_hot,'Exercise Type')\n",
    "                    .pipe(ordinal_encode, 'Quality of Exercise')\n",
    "            )\n",
    "exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb9b30",
   "metadata": {},
   "source": [
    "## Using classes and objects\n",
    "(Working on this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LoadExercise():\n",
    "    def __init__(self, url):\n",
    "        self.data = pd.read_csv(url)\n",
    "\n",
    "    def clean_data(self):\n",
    "        # Drop column\n",
    "        self.data.drop('Date', axis=1, inplace=True) \n",
    "        # Function to fill in missing values\n",
    "        self.data['Calories'].replace(np.nan, self.data['Calories'].mean(), inplace=True) \n",
    "        # One-hot encode categorical data\n",
    "        self.data = self.data.join(pd.get_dummies(self.data['Exercise Type']).astype(int)).drop('Exercise Type', axis=1) \n",
    "        # Ordinal encode categorical data\n",
    "        order = {\n",
    "            'None':0,\n",
    "            np.nan:0,\n",
    "            'Low':1,\n",
    "            'Medium':2,\n",
    "            'High':3\n",
    "        }\n",
    "        self.data['Quality of Exercise'] = self.data['Quality of Exercise'].map(order)\n",
    "\n",
    "exercise = LoadExercise('Data/exercise.csv')\n",
    "display(exercise.data.head())\n",
    "\n",
    "exercise.clean_data()\n",
    "display(exercise.data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c96296",
   "metadata": {},
   "source": [
    "## Using *sci-kit learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ed607",
   "metadata": {},
   "source": [
    "*Scikit-learn* has a number of packages to do these preprocessing tasks. These functions have a lot of features that do the job more effectively and cleanly, so is a better option than our self-made functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a731cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables\n",
    "exercise = pd.read_csv('Data/exercise.csv')\n",
    "X = exercise.drop(['Date','Weight Lost'], axis=1).values\n",
    "\n",
    "# Ordinal Encoder won't like nan values. Change to 'None'\n",
    "# This fits with data since there was 0 activity for that day\n",
    "X[:,3] = ['None' if x is np.nan else x for x in X[:,3]]\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(exercise['Weight Lost'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d625e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Missing Values\n",
    "## Calories = Column 0\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:,0:1])\n",
    "X[:,0:1] = imputer.transform(X[:,0:1])\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52886e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Encode Nominal Variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "onehot.fit_transform(X[:,1:2]).toarray()\n",
    "\n",
    "# Columns are in Alphabetical Order\n",
    "# 1st Column = Running\n",
    "# 2nd Column = Stairs\n",
    "# 3rd Column = Swimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encode Ordinal Variables\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder(categories=[['None','Low','Medium','High']])\n",
    "oe.fit_transform(X[:,3].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaec903",
   "metadata": {},
   "source": [
    "#### Piping functions in one command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceced8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Encode nominal variables and Ordinal Encode\n",
    "# ordinal variables but keep all variables\n",
    "\n",
    "# Reload Data and set up variables\n",
    "exercise = pd.read_csv('Data/exercise.csv')\n",
    "X = exercise.drop(['Date','Weight Lost'], axis=1).values\n",
    "\n",
    "# Ordinal Encoder won't like nan values. Change to 'None'\n",
    "# This fits with data since there was 0 activity for that day\n",
    "X[:,3] = ['None' if x is np.nan else x for x in X[:,3]]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# When putting in the columns in each imputer/encoder, indicate the column\n",
    "# of the original matrix\n",
    "  # [0]: Calories - fill missing values\n",
    "  # [1]: Exercise Type - One-hot encoding\n",
    "  # [3]: Quality of Exercise - Ordinal encoding\n",
    "\n",
    "ct = ColumnTransformer(transformers=[\n",
    "      ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean'), [0]),  # This is placed first in X\n",
    "      ('onehot', OneHotEncoder(), [1]),                                         # This is placed second in X\n",
    "      ('oe', OrdinalEncoder(categories=[['None','Low','Medium','High']]), [3])  # This is placed third in X\n",
    "    ], remainder='passthrough')                     # Remaining columns placed in order after the last encoder\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(ct.fit_transform(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d48f1c",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "897ddcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a411adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb02bc",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "The scales of variables can have a very large impact on the results of the model. For instance, consider this example of employee salaries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa02ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "salaries = {\n",
    "    'ID':['01','02','03'],\n",
    "    'Salary':[70000,60000,52000],\n",
    "    'Years of Experience':[5,4,1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(salaries, index=salaries['ID']).drop('ID', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa8a38",
   "metadata": {},
   "source": [
    "We want to group employees together. Employees 1 and 3 are definitely in different groups. But how would we group Employee 2? Employee 2 is closer to Employee 1 in salary, but to Employee 3 in experience. \n",
    "\n",
    "The scale is throwing us off, so we look at __feature scaling__. There are two methods of feature scaling:\n",
    "1. Standardization\n",
    "$$\\hat{x} = \\frac{x-\\bar{x}}{s}$$\n",
    "2. Min-Max Scaling\n",
    "$$\\hat{x} = \\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "3. Normalization\n",
    "$$\\hat{x} = \\frac{x-\\bar{x}}{x_{max}-x_{min}}$$\n",
    "\n",
    "Standardization will generally give a number in the range [-3,3] (outliers will be more extreme than that), while min-max scaling and normalization will always give a result between [0,1].\n",
    "\n",
    "Let's see how each method affects the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eae03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "def normalize_df(x):\n",
    "    return (x-x.mean())/(x.std(ddof=1))\n",
    "\n",
    "normalize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "def standardize_df(x):\n",
    "    return (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "standardize_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1f41d",
   "metadata": {},
   "source": [
    "What do we see? Looking at the original data, the gap in salaries between Employees 1 and 2 was so large that we'd say that Employee 2 was closer to Employee 3. But as we look at the standardized and normalized data, we see that the salary of Employee 2 is very nearly in the middle (0 for Standardized, 0.5 for Normalized). So, the Salary may not be a good indicator. But looking at the Years of Experience, we see Employee 2 is actually very close to Employee 1. So, it is more likely for Employee 2 to be grouped with Employee 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc52bba9",
   "metadata": {},
   "source": [
    "-----\n",
    "# Class Project 1\n",
    "There was a survey completed asking young people a few questions regarding preferences. Here is a quick explanation of the dataset\n",
    "* The ['Music', 'Techno', 'Movies', 'History', 'Mathematics', 'Pets', 'Spiders'] columns indicate how much the person likes or dislikes each category on a scale of 1-5\n",
    "* The ['Loneliness'] column indicates how lonely a person feels on a scale of 1-5\n",
    "* The ['Parents Advice'] column indicates how much the person appreciates advice from parents on a scale of 1-5\n",
    "* The ['Internet usage'] column indicates how much time is spent online on a scale of 1-5\n",
    "* The ['Finances'] column indicates how stable the person is financially on a scale of 1-5\n",
    "* The ['Age'] column is the person's age\n",
    "* The ['Siblings'] column is the number of siblings the person has\n",
    "* The ['Gender'] is male/female\n",
    "* The ['Village - town'] indicates whether the person lives in the city (urban living) or in a village (rural living)\n",
    "\n",
    "We are going to use the dataset to create a model to predict whether a person is likely to be lonely or not. Your job in this project is to complete the entire data preprocessing for the data.\n",
    "\n",
    "* Load the *young-people-survey-responses* dataset\n",
    "  * Located on the [github page](https://github.com/drolsonmi/math3480)\n",
    "* Perform Data Preprocessing on this data\n",
    "  * What variables are not needed? Drop them\n",
    "  * Handle missing values \n",
    "    * If more than 10% of the values in a given row/column are missing, remove them\n",
    "    * If fewer than 10% of the values in a given row/column are missing, fill them with min, max, mean, or median - whatever will best deal with each variable\n",
    "  * Encode categorical variables using either one-hot encoding, ordinal encoding, or label encoding\n",
    "* Divide the data into Training and Testing Groups\n",
    "* Scale the features by using standardized scaling\n",
    "  * Do all features need to be scaled? Consider each variable carefully.\n",
    "* Apply to a logistic regression model to see if you can model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d3dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
