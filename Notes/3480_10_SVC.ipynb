{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 Support Vector Classifiers\n",
    "__MATH 3480__ - Dr. Michael Olson\n",
    "\n",
    "Reading:\n",
    "* Geron, Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Machine Learning Landscape](https://raw.githubusercontent.com/drolsonmi/math3480/main/Notes/Images/3480_05_ML_Landscape.png)\n",
    "\n",
    "## The Concept behind Support Vector Classifiers\n",
    "* Two datasets have a gap between them\n",
    "* Draw a line to separate the datasets\n",
    "  * The distance from the closest datapoint to the separator is known as the __margin__\n",
    "  * When the separator is in the middle, the margin is maximized for both datasets. This is known as the __maximal margin classifier__ (mmc)\n",
    "  * This margin has two problems:\n",
    "    1. Only works if data is linearly separable\n",
    "    2. Sensitive to outliers - If you have a datapoint from one dataset that is near the other dataset, the mmc is decreased and misplaced. New datapoints near the second dataset could be classified in the first dataset\n",
    "* Bias/Variance tradeoff\n",
    "  * If we force all points to be correct, we have *low bias*. However, this overfits the data, so our predictions will often be incorrect, giving us a *high variance*\n",
    "  * If we allow misclassifications, we have *high bias*, but the predictions are more accurate, giving us a *low variance*\n",
    "* Allow misclassifications (or *margin violations*)\n",
    "  * When we allow misclassifications, then we call that margin a __soft margin__\n",
    "* Determine best soft margin\n",
    "  * Use cross validation\n",
    "  \n",
    "Using a soft margin is a machine learning model known as a __soft margin classifier__, more commonly known as a __Support Vector Classifier__ (SVC)\n",
    "* With 2-dimensional data, the SVC is a line\n",
    "* With 3-dimensional data, the SVC is a plane\n",
    "* ...\n",
    "* With n dimensions, the SVC has n-1 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(#iris['data'][50:,0],\n",
    "           iris['data'][50:,2],\n",
    "           iris['data'][50:,3],\n",
    "           c=iris['target'][:100])\n",
    "          \n",
    "ax.set_xlabel('Petal Length (cm)')\n",
    "ax.set_ylabel('Petal Width (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. Missing Data - No missing values in this example\n",
    "2. Encode Categorical Variables - Using original data, no categorical variables\n",
    "3. Split the data\n",
    "4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data'][:,(2,3)]\n",
    "y = (iris['target'] == 2).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear_svc', LinearSVC(C=1, loss='hinge'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of hyperparameters:\n",
    "* `C` is the regularization parameter - determines the number of misclassifications\n",
    "  * High C means we regularize more (allow fewer misclassifications) - smaller margins\n",
    "  * Low C means we regularize less (allow more misclassifications) - larger margins\n",
    "* `loss` is the loss function\n",
    "  * None selected by default - we have to set one to run the model\n",
    "  * `hinge` is the typical loss type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
