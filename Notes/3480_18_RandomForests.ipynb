{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1a6b73-bc2f-477e-9dac-4cda7bb88143",
   "metadata": {},
   "source": [
    "# Improving Models\n",
    "## Boosting, Random Forests, ADA Boost, Gradient Boost\n",
    "#### Math 3480 - Machine Learning - Dr. Michael E. Olson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b337c",
   "metadata": {},
   "source": [
    "## Reading\n",
    "* Geron, Chapter 7\n",
    "\n",
    "### Additional Resources\n",
    "* [YouTube StatQuest: Random Forests](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&t=214s)\n",
    "* [YouTube StatQuest: ADABoost](https://www.youtube.com/watch?v=LsK-xG1cLYA&t=211s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739e7aa",
   "metadata": {},
   "source": [
    "Decision Trees are efficient, but often have poor results. To improve the results, we aggregate the information, or look at groups of predictors. This group of predictors is called an __ensemble__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790f904",
   "metadata": {},
   "source": [
    "### Voting\n",
    "One way we can get the best results is to run the data through the different methods (Logistic Regression, SVM, Random Forests, KNN,...) and take the model that performs the best\n",
    "* Run the data through each ML algorithm\n",
    "* Aggregate (or collect) the data together\n",
    "* Take a majority vote on which method has the most correct classifications, and aggregate the results\n",
    "\n",
    "The majority-vote classifier is called *hard voting*.\n",
    "\n",
    "*Soft voting* would be predicting the class with the highest class probability averaged over all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818048f0",
   "metadata": {},
   "source": [
    "### Bagging (Bootstrap AGGregatING)\n",
    "To bootstrap (or create a subset of your dataset):\n",
    "* Take a random element from your data, and include it in your dataset\n",
    "  * with replacement (it's not removed from the original dataset, so it could be selected again later)\n",
    "* Continue to take a determined number of elements from your dataset, with replacement\n",
    "  * If you repeat some elements, that is just fine - it is useful in the algorithm\n",
    "  * How many elements in each bag?\n",
    "    * If $n$ is the number of elements in the original dataset, and $n'$ is the number in the bag, we want $n'<n$\n",
    "    * A good round figure would be $n' \\approx 60\\% \\cdot n$. Not a hard number - can vary based on need\n",
    "\n",
    "We will then use each bag to be trained into models. We then apply our data to each model and take the average for our result. This is the __ensemble__ process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83d77a",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "In bagging, we created a bootstrapped dataset allowing duplicate entries in each bag. This means that, on average, roughly 63% of the data is used to make bags. The remaining 37% are *out-of-bag instances*.\n",
    "\n",
    "__Out-of-Bag Evaluation__ takes the 37% of data that wasn't used and runs them through the tree the bag created.\n",
    "* If the results of the tree are the same as the data's results, then it was successful\n",
    "* If the results of the tree are different, then \n",
    "\n",
    "You can find how well the out-of-bag instances performed in the tree by setting `oob_score=True` in your model, then printing `oob_score_`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e4ef8",
   "metadata": {},
   "source": [
    "#### How to do Bagging in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 6)\n",
    "from sklearn import datasets\n",
    "\n",
    "iris_data = datasets.load_iris()\n",
    "iris = pd.DataFrame(iris_data['data'], columns=iris_data['feature_names'])\n",
    "species = pd.DataFrame(iris_data['target'], columns=['species_num'])\n",
    "\n",
    "def test_species(x):\n",
    "    if x==0: return \"setosa\"\n",
    "    if x==1: return \"versicolor\"\n",
    "    if x==2: return \"virginica\"\n",
    "\n",
    "iris['species'] = species['species_num'].apply(lambda x: test_species(x))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.drop(['species'],axis=1), iris['species'], test_size=0.3, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# n_estimators :    Number of models to make\n",
    "# max_samples :     Number of instances in each bag\n",
    "# bootstrap :       If True, bootstraps with replacement. If False, without replacement\n",
    "# n_jobs :          Number of processors to use (-1 means use all processors)\n",
    "# oob_score :       Calculate Out-of-Bag Evaluation\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, max_samples=100, \n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "    \n",
    "bag_clf.fit(X_train, y_train)\n",
    "print(\" OOB Score = \",bag_clf.oob_score_)\n",
    "y_predict = bag_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\" Accuracy Score = \", accuracy_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16d163",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "The idea of Random Forests is to create a number of decision trees, and test your data with all trees.\n",
    "* Round 1:\n",
    "  * Bootstrap your data\n",
    "  * Using the bootstrapped data, select only a few variables to test\n",
    "  * Determine which variable creates the best split in the data - this is your first node\n",
    "  * Select only a few of the remaining variables to test next\n",
    "  * The variable that creates the best split becomes the second node\n",
    "  * ...etc...\n",
    "* Round 2:\n",
    "  * Repeat the same steps\n",
    "  * The result will be a second tree\n",
    "* Round 3:\n",
    "  * Repeat again\n",
    "* ...\n",
    "* Do this for however many rounds we want (generally 100's of times)\n",
    "\n",
    "The result of this process is a large number of Decision Tree models, which we call a *Random Forest*.\n",
    "\n",
    "How does it work? We run new data down each Decision Tree. We count the results, and the result that is most common is our classification (Voting method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9589ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 6)\n",
    "from sklearn import datasets\n",
    "\n",
    "iris_data = datasets.load_iris()\n",
    "iris = pd.DataFrame(iris_data['data'], columns=iris_data['feature_names'])\n",
    "species = pd.DataFrame(iris_data['target'], columns=['species_num'])\n",
    "\n",
    "def test_species(x):\n",
    "    if x==0: return \"setosa\"\n",
    "    if x==1: return \"versicolor\"\n",
    "    if x==2: return \"virginica\"\n",
    "\n",
    "iris['species'] = species['species_num'].apply(lambda x: test_species(x))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.drop(['species'],axis=1), iris['species'], test_size=0.3, random_state=32)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "   # n_estimators = number of trees\n",
    "rfc.fit(X_train,y_train)\n",
    "rfc_predict = rfc.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test,rfc_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa8c9f",
   "metadata": {},
   "source": [
    "Another method for improving our models is to evaluate the results, then add a little more weight to the incorrectly labelled data. There are a number of processes to do this. We're focusing on two: __AdaBoost__ and __Gradient Boost__.\n",
    "\n",
    "### AdaBoost\n",
    "With AdaBoost, we start with a decision tree that has only one node (and two leaves).\n",
    "* A tree with only one node and two leaves is called a __stump__.\n",
    "* Note that since a decision is made with only one variable, a stump is a *weak learner*.\n",
    "* But this doesn't matter. We will take the weak learners and make them stronger.\n",
    "\n",
    "We make a stump for each variable. We run through the different stumps and classify all the data, and give each instance a weight.\n",
    "1. In the first round, each of the *m* instances has the same weight *1/m*\n",
    "  * Run the data through the first stump (separate by the first variable)\n",
    "  * Track the number of correct and incorrect classifications using the Gini index\n",
    "  * Do the same for all stumps\n",
    "  * Use these results to create a new weight\n",
    "    * Calculate the total error $r_j$\n",
    "    * Calculate the Predictor Weight $\\alpha_j$\n",
    "    * Calculate and normalize the new weight $w^{(i)}$\n",
    "\n",
    ">$$r_j = \\frac{\\sum_{i=1; \\hat{y}_j^{(i)}\\ne y^{(i)}}^m w^{(i)}}{\\sum_{i=1}^m w^{(i)}}$$\n",
    ">$$\\alpha_j = \\eta \\log\\frac{1-r_j}{r_j}$$\n",
    ">$$w^{(i)} \\leftarrow \\begin{cases}\n",
    "w^{(i)} & \\text{if }\\hat{y}_j^{(i)} = y^{(i)}\\\\\n",
    "w^{(i)}e^{\\alpha_j} & \\text{if }\\hat{y}_j^{(i)}\\ne y^{(i)}\n",
    "\\end{cases} $$\n",
    ">* $\\hat{y}_j^{(i)}$ is the predition for the j-th predictor for the i-th instance\n",
    ">* $\\eta$ is the learning rate hyperparameter\n",
    "\n",
    "This is what the Predictor Weight $\\alpha_j$ looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d480c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.arange(0,1,0.01)\n",
    "y = np.log((1-x)/x)\n",
    "plt.plot(x,y)\n",
    "plt.title('Predictor Weight')\n",
    "plt.xlabel('Total Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa1e14",
   "metadata": {},
   "source": [
    "* If the total error is very small, then the predictor weight is laregely positive (guesses the correct answer)\n",
    "* If the total error is very large, then the predictor weight is laregely negative (guesses the incorrect answer)\n",
    "* If the total error is in the middle, then the predictor weight is about 0 (no better than random guessing)\n",
    "\n",
    "We then repeat these steps with the updated weights. We keep repeating this process until either the desired number of predictors is reached, or when a perfect predictor is found.\n",
    "\n",
    "Let's run through an example with ten data instances ${x_0,...,x_9}$ and six variables ${y_0,...,y_5}$.\n",
    "1. Weights for all instances = $1/10 = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf035f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 10\n",
    "w = np.ones(n)/n\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa586eb8",
   "metadata": {},
   "source": [
    "2. Run the data for all six variables. We find the Gini index is best for $y_3$.\n",
    "    * Creating a stump for $y_3$ produces two errors: $x_5$ and $x_8$ were incorrectly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a929536",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.ones(10)\n",
    "error[5] = -1\n",
    "error[8] = -1\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4e1b9",
   "metadata": {},
   "source": [
    "3. Calculate the total error\n",
    "    * $r_1 = \\frac{2}{10}=0.2$\n",
    "4. Calculate the Predictor Weight $\\alpha_1$\n",
    "    * For now, let's set a learning rate of 1/2\n",
    "    * $\\alpha_1 = 0.5\\log\\frac{1-0.2}{0.2} = 0.693$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sum(-0.5*(error-1))/n\n",
    "print(\"r = \",r)\n",
    "\n",
    "a = 0.5*np.log( (1 - r) / r )\n",
    "print(\"a = \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ed221",
   "metadata": {},
   "source": [
    "5. Find the new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w*np.exp(-error*a)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d287c",
   "metadata": {},
   "source": [
    "6. Normalize the new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w/sum(w)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320bc15",
   "metadata": {},
   "source": [
    "You run this through the decision trees to get the next stump using either\n",
    "* the Weighted Gini Index, or \n",
    "* create a new dataset the same size as the original, repeating the heavily weighted instances\n",
    "  * Create a Total Gathered Weight $gw_k = \\sum_{i=0}^k w_i$ for each element $k$.\n",
    "  * Choose a random number between 0 and 1\n",
    "  * If that number falls in the range $(gw_{k-1},gw_k)$, then the $k$th element is put into the dataset\n",
    "    * Because the misclassified instance has a high weight, it will likely be chosen multiple times\n",
    "  * For the new dataset, give each instance the same weight $1/m$\n",
    "    * Since the misclassified instances may appear multiple times, then it's weight will essentially be $1/m$ multiplied by the number of times it appears in the new dataset\n",
    "\n",
    "Run the steps again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8\n",
    "w = np.ones(n)/n\n",
    "\n",
    "error = np.ones(n)\n",
    "error[3] = -1\n",
    "#error[8] = -1\n",
    "\n",
    "r = sum(-0.5*(error-1))/n\n",
    "print(\"r = \",r)\n",
    "a = 0.5*np.log( (1 - r) / r )\n",
    "print(\"a = \",a)\n",
    "\n",
    "w = w*np.exp(-error*a)\n",
    "w/sum(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509aa00",
   "metadata": {},
   "source": [
    "To determine the classification for new data, run it through each stump. Add the $\\alpha_j$ values for all stumps that came out in one category, and add all $\\alpha_j$ values for the other category. The category with the higher sum wins, and is the classification for that data.\n",
    "$$\\hat{y}(x) = \\argmax_k \\sum_{j=1; \\hat{y}_j(x)=k}^N \\alpha_j$$\n",
    "where $N$ is the number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 6)\n",
    "from sklearn import datasets\n",
    "\n",
    "iris_data = datasets.load_iris()\n",
    "iris = pd.DataFrame(iris_data['data'], columns=iris_data['feature_names'])\n",
    "species = pd.DataFrame(iris_data['target'], columns=['species_num'])\n",
    "\n",
    "def test_species(x):\n",
    "    if x==0: return \"setosa\"\n",
    "    if x==1: return \"versicolor\"\n",
    "    if x==2: return \"virginica\"\n",
    "\n",
    "iris['species'] = species['species_num'].apply(lambda x: test_species(x))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.drop(['species'],axis=1), iris['species'], test_size=0.3, random_state=32)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = ada_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40efbeb",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "Another way to improve model performance is using __Gradient Boosting__. Gradient boosting is applied to both Regression and Classification models. First, let's look at regression models:\n",
    "\n",
    "This method is very similar to AdaBoost, but instead of changing weights every time, it looks at the residual errors made by the previous predictor. Here's how it works:\n",
    "1. Make an initial prediction for all data (generally, the average of the observed values)\n",
    "2. Find the residuals (actual - predicted)\n",
    "3. Classify all these residuals in a decision tree\n",
    "    * Generally, we create a tree with anywhere between 4 and 32 leaves\n",
    "    * If we have multiple values in a given leaf, then average all values in that leaf\n",
    "4. Add the initial prediction in (1) to the classification in the decision tree in (3)\n",
    "    * *Note*: Doing this will actually overfit the data and creates high variance in predictions\n",
    "    * To prevent this, we scale this prediction (multiply it by a number between 0 and 1). This is where the gradient descent happens\n",
    "5. Repeat steps 2-4 using the new predictions and new residuals\n",
    "6. Continue repeating steps 2-4 until we reach a maximum number of trees or until additional trees stop improving the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f3412",
   "metadata": {},
   "source": [
    "##### Math for the Gradient Boosting\n",
    "__Input__: \n",
    "* Data $\\{(x_i,y_i)\\}_{i=1}^n$\n",
    "* Differentiable __Loss Function__ $L(y_i,F(x))$\n",
    "  * Often, we take,\n",
    "  $$L(y_i,F(x)) = \\frac{1}{2}(Observed-Predicted)^2$$\n",
    "__Step 1__:\n",
    "* Initialize model with a constant value:\n",
    "$$F_0(x)=\\argmin_\\gamma \\sum_{i=1}^n L(y,\\gamma)$$\n",
    "* $\\gamma$ is just the predicted value\n",
    "* We can find the $\\argmin$ by taking the derivative of the loss function and setting it equal to 0 to minimize the errors\n",
    "$$\\frac{dL}{d Predicted} = -(Observed-Predicted)=0$$\n",
    "__Step 2__:\n",
    "```python\n",
    "for m=1 to M:\n",
    "```\n",
    "1. Compute residuals $r_{im} = -\\left[\\frac{\\partial L}{\\partial F}\\right]_{F(x)=F_{m-1}(x)}$ for $i={1\\dots,n}$. \n",
    "    * This is just the residual $(Observed-Predicted)$\n",
    "    * In practice, $M=100$, but can vary\n",
    "2. Fit a regression tree to the $r_{im}$ values and create terminal region $R_{jm}$ for $j=1\\dots J_m$.\n",
    "    * This is the value for each leaf in the decision tree\n",
    "3. For $j=1\\dots J_m$, compute $\\gamma_{jm} = \\argmin_\\gamma \\sum_{x_i\\in R_{ij}} L(y_i,F_{m-1}(x_i)+\\gamma)$\n",
    "    * This produces the average of the residuals in each leaf\n",
    "4. Update $F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_{jm}I(x\\in R_{jm})$\n",
    "    * This is the new prediction\n",
    "    * $\\nu$ is the learning rate\n",
    "__Step 3__:\n",
    "* Output $F_M(x)$\n",
    "\n",
    "Then, this model is used with any new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f50fd",
   "metadata": {},
   "source": [
    "\n",
    "First, run the model. Make a prediction, then calculate the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51fe65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "### First prediction can be done as an average\n",
    "y0 = np.mean(y_train)*np.ones_like(y_train)\n",
    "### or using a decision tree\n",
    "#tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "#y0 = tree_reg1.fit(X_train, y_train)\n",
    "\n",
    "r0 = y - y0 # Residuals from first esimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf29a54",
   "metadata": {},
   "source": [
    "Then, train and fit a new model to the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97232452",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X_train, r0)\n",
    "\n",
    "y1 = tree_reg1.predict(X_train) \n",
    "r1 = y - y1 # Residuals from first tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b311d66",
   "metadata": {},
   "source": [
    "Keep going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg2= DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X_train, r1)\n",
    "\n",
    "y2 = tree_reg2.predict(X_train)\n",
    "r2 = y - y2 # Residuals from third tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde16e9c",
   "metadata": {},
   "source": [
    "You can keep going as far as is necessary.\n",
    "\n",
    "The prediction will then be the sum of the predictions in each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85538045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X = np.linspace(0,20,100)\n",
    "y = X**2 - 4*X + 15.3 + 40**np.random.random(len(X))\n",
    "\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec96a0-b1f0-47da-ac38-e98624a04214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y,label=\"Data\")\n",
    "\n",
    "L_rate = 1.0\n",
    "L_rate = 0.6\n",
    "\n",
    "# Initial Prediction\n",
    "y0 = np.mean(y)*np.ones_like(y)\n",
    "y_predict = y0\n",
    "plt.plot(X,y0,color=\"red\",label=\"1st prediction\")\n",
    "\n",
    "# 1st Tree\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X.reshape(-1,1), y-y0)\n",
    "\n",
    "y1 = tree_reg1.predict(X.reshape(-1,1))\n",
    "y_predict += L_rate*y1\n",
    "plt.plot(X,y_predict,color=\"orange\", label=\"2nd prediction\")\n",
    "\n",
    "# 2nd Tree\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X.reshape(-1,1), y-y_predict)\n",
    "\n",
    "y2 = tree_reg2.predict(X.reshape(-1,1))\n",
    "y_predict += L_rate*y2\n",
    "plt.plot(X,y_predict,color=\"yellow\", label=\"3rd prediction\")\n",
    "\n",
    "# 3rd Tree\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X.reshape(-1,1), y-y_predict)\n",
    "\n",
    "y3 = tree_reg3.predict(X.reshape(-1,1))\n",
    "y_predict = y0 + L_rate*y3\n",
    "plt.plot(X,y_predict,color=\"green\", label=\"4th prediction\")\n",
    "\n",
    "# # 4th Tree\n",
    "tree_reg4 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg4.fit(X.reshape(-1,1), y-y_predict)\n",
    "\n",
    "y4 = tree_reg4.predict(X.reshape(-1,1))\n",
    "y_predict = y0 + L_rate*y4\n",
    "plt.plot(X,y_predict,color=\"cyan\", label=\"5th prediction\")\n",
    "\n",
    "# 5th Tree\n",
    "tree_reg5 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg5.fit(X.reshape(-1,1), y-y_predict)\n",
    "\n",
    "y5 = tree_reg5.predict(X.reshape(-1,1))\n",
    "y_predict = y0 + L_rate*y5\n",
    "plt.plot(X,y_predict,color=\"purple\", label=\"6th prediction\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4e47c-efc8-4300-9a34-7733ea05de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y,label=\"Data\")\n",
    "\n",
    "L_rate = 1.0\n",
    "#L_rate = 0.6\n",
    "\n",
    "# Initial Prediction\n",
    "y0 = np.mean(y)*np.ones_like(y)\n",
    "y_predict = y0\n",
    "plt.plot(X,y0,color=\"red\",label=\"Prediction #1\")\n",
    "\n",
    "# Trees\n",
    "tree1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree4 = DecisionTreeRegressor(max_depth=2)\n",
    "tree5 = DecisionTreeRegressor(max_depth=2)\n",
    "\n",
    "n = 2\n",
    "for i in [tree1, tree2, tree3, tree4, tree5]:\n",
    "    i.fit(X.reshape(-1,1), y-y0)\n",
    "    yi = i.predict(X.reshape(-1,1))\n",
    "    y_predict += L_rate*yi\n",
    "    plt.plot(X,y_predict,label=\"Prediction #{0}\".format(n))\n",
    "    n += 1\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3e74b-7c51-4623-9fc8-dab6efaf21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=5, learning_rate=0.6)\n",
    "gbr.fit(X.reshape(-1,1),y)\n",
    "y_predict = gbr.predict(X.reshape(-1,1))\n",
    "\n",
    "plt.scatter(X,y,label=\"Data\")\n",
    "plt.plot(X,y_predict,label=\"Prediction\",color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901548d0-d19f-44dc-b37f-f8ad42e43392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8a940844f7f8998522eedb112310c0958420b6c917d0853ef275f15829120cc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
